# 🌪️ DiffTTS: Denoising Diffusion Probabilistic Model for TTS

---

## 📌 개요

**DiffTTS**는 최초로 diffusion probabilistic model을 **TTS의 mel-spectrogram 생성**에 적용한 모델입니다.  
특징 요약:

| 항목                  | 내용 |
|-----------------------|------|
| 🎯 목적                | 텍스트로부터 자연스러운 mel-spectrogram 생성 |
| 🌀 방법                | 노이즈에서 시작해 점진적으로 denoise하여 mel 생성 |
| 🏗️ 구성                | text encoder + duration + diffusion decoder |
| 🚫 attention 없음      | alignment-free (duration만 사용) |

---

## 🔁 작동 흐름 요약

```text
Text → Phoneme Encoder → Duration Predictor → z₀ (noise) → ... → Mel
```

훈련 시:
```text
Mel → Forward Noise → z_t
    ↓
text 조건과 함께 denoise 학습
```

추론 시:
```text
z_T ~ N(0, I) → text 조건 기반 denoise → Mel → Vocoder → Waveform
```

---

## 🧠 Diffusion 수식 (기초)

### 🔹 Forward process (노이즈 추가)
```math
q(x_t | x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t} x_0, (1 - \bar{\alpha}_t) I)
```

### 🔹 Reverse process (denoise)

```math
x_{t-1} = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \cdot \epsilon_\theta(x_t, t, c) \right)
```

여기서:

- $( x_t $): noisy mel
- $( c $): 텍스트 조건 (text encoder 출력)
- $( \epsilon_\theta $): 현재 상태에서 노이즈를 예측하는 신경망 (U-Net 등)

---

## 🧪 예시 데이터 흐름 (Step-by-Step)

### 1. 입력 텍스트: `"hi"`
```python
text = "hi"
phonemes = ['HH', 'AY']
text_embed = text_encoder(phonemes)  # [1, 2, D]
```

### 2. Duration 예측 → Length Regulate
```python
dur = [3, 5]  # 예: HH = 3프레임, AY = 5프레임
text_expanded = length_regulator(text_embed, dur)  # → [1, 8, D]
```

### 3. GT mel → forward noise (학습 시)
```python
mel_gt = extract_mel(waveform)  # [1, 8, 80]
t = random.randint(1, T)
noise = torch.randn_like(mel_gt)
x_t = sqrt_alpha_bar[t] * mel_gt + sqrt_one_minus_alpha_bar[t] * noise
```

### 4. 모델 학습: 노이즈 예측
```python
pred_noise = epsilon_theta(x_t, t, cond=text_expanded)
loss = F.mse_loss(pred_noise, noise)
```

### 5. 추론 시: z_T → mel
```python
z_t = torch.randn([1, 8, 80])
for t in reversed(range(1, T+1)):
    pred_noise = epsilon_theta(z_t, t, cond=text_expanded)
    z_t = reverse_step(z_t, pred_noise, t)
```

---

## 🧩 DiffTTS 주요 구성 모듈

| 모듈                  | 설명 |
|------------------------|------|
| **Text Encoder**       | 음소 임베딩 및 contextual encoding (Conv + attention 등) |
| **Duration Predictor** | 정렬을 위한 멜 프레임 수 예측 |
| **Length Regulator**   | 텍스트 임베딩을 멜 길이에 맞게 복제 |
| **Denoiser (UNet)**    | 각 step의 노이즈 제거 예측기 |
| **Vocoder**            | mel → waveform (HiFi-GAN 등 사용) |

---

## 📈 Loss 구성

```text
L_total = E[ || ε - ε_θ(x_t, t, c) ||^2 ] + duration loss
```

- Diffusion Loss: 노이즈 예측 정밀도
- Duration Loss: 예측된 프레임 수 정확도

---

## 🧠 DiffTTS vs Grad-TTS vs Glow-TTS

| 항목        | DiffTTS               | Grad-TTS              | Glow-TTS                |
|-------------|------------------------|------------------------|--------------------------|
| 구조        | 가장 단순한 diffusion | latent space diffusion | flow 기반 invertible     |
| 정렬 방식   | duration 사용          | duration 사용          | monotonic alignment search |
| 품질        | 좋음 (prosody 다양성) | 더 좋음 (운율 자연스러움 ↑) | 빠름, 품질 좋음           |
| 추론 속도   | 느림 (T step 반복)     | 느림                   | 빠름 (1 pass)            |
| 용도 적합성 | 연구, 기반 실험용      | 고품질 음성 생성       | 빠른 실서비스에 적합      |

---

## 📘 논문 정보

- **Title**: Diff-TTS: A Denoising Diffusion Model for Text-to-Speech
- **Authors**: Kim et al., 2020
- **Paper**: https://arxiv.org/abs/2104.01409
- **Code**: https://github.com/ericwang2006/Diff-TTS-PyTorch

---

## ✅ 정리 요약

| 항목          | 설명                                  |
|---------------|---------------------------------------|
| 목적          | Text → Mel with diffusion             |
| 훈련 방식      | GT mel + noise → denoise prediction    |
| 추론 방식      | noise → mel (step-by-step sampling)   |
| 정렬 방식      | duration predictor 사용               |
| 특징           | attention-free, 높은 품질, 느린 속도  |

---

> 🌀 DiffTTS는 **TTS + Diffusion의 첫 결합 실험**으로,  
> 이후 Grad-TTS, StyleTTS2, FastDiff,
