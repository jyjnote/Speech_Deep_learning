# 🔄 Stage 3: End-to-End TTS 모델 총정리

---

## 📌 개요

전통적인 TTS 파이프라인:
```
Text → Phoneme → Acoustic Feature → Vocoder → Waveform
```
➡️ 복잡하고 각 모듈이 따로 학습됨.

---

**End-to-End TTS 모델**은 위 모든 단계를 **하나의 모델로 통합**하여 다음을 달성:

- 더 적은 전처리
- 음질/일관성 개선
- 빠른 추론

---

## 📊 주요 모델 비교

| 모델         | 핵심 구성 요소         | 생성 방식       | 특징                                 |
|--------------|-------------------------|------------------|--------------------------------------|
| **VITS**     | VAE + Flow + GAN         | 샘플링 + GAN     | 품질 최고, 실시간 어려움             |
| **Glow-TTS** | Flow 기반 (non-autoregressive) | 샘플링          | 자연스러운 prosody, 병렬 생성 가능   |
| **Grad-TTS** | Diffusion 기반           | 노이즈 제거형    | 안정적 음질, 느린 속도               |
| **DiffTTS**  | Diffusion + duration pred| 노이즈 제거형    | non-autoregressive, 품질 향상        |

---

# 📌 1. VITS (VAE + Flow + GAN 통합형)

> **V**ariational **I**nference + **T**TS + GAN

### 🧬 구조 요약

```text
Text → Posterior Encoder → Latent z
↓                              ↑
Decoder (Vocoder) ← Prior (Flow)
↓
Waveform
```

### ✅ 특징

- Variational Autoencoder 구조 사용 (posterior vs prior)
- Prior는 Text로부터 latent z 분포를 예측
- Decoder는 z로 waveform 직접 생성 (HiFi-GAN 기반)
- Flow를 통해 음소별 alignment 없이 학습 가능

### 🧠 학습 목표

```math
L = Reconstruction Loss + KL Divergence + Adversarial Loss + Duration Loss
```
# 📐 Kullback–Leibler Divergence (KL Divergence)

---

## 📌 정의

KL Divergence는 **두 확률 분포** \( P(x), Q(x) \) 사이의 비대칭적인 거리입니다.

```math
D_{KL}(P || Q) = \sum_x P(x) \log \frac{P(x)}{Q(x)}
```

- \( P(x) \): 실제 분포 (예: posterior, 인코더 출력)
- \( Q(x) \): 근사 분포 (예: prior, 정규분포)

---

## 🧠 직관

- KL Divergence는 "**Q 분포가 P를 얼마나 잘 근사하는가**"를 측정합니다.
- 값이 작을수록 두 분포가 비슷함.
- **0이면 완전히 같음.**

---

## 🧪 예시: 정규분포 간 KL 계산

```python
import torch
import torch.nn.functional as F

# 인코더에서 얻은 posterior 분포 (mu, logvar)
mu = torch.tensor([0.5])
logvar = torch.tensor([0.1])  # log(σ^2)

# KL Divergence from N(mu, σ^2) to N(0,1)
kld = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
print(f"KL Divergence: {kld.item():.4f}")
```

---

## 🔁 TTS에서의 KL Divergence

### ✅ 사용 위치

| 모델   | KL Divergence 사용 용도                                  |
|--------|-----------------------------------------------------------|
| **VAE** | `posterior q(z|x)` vs `prior p(z)`                       |
| **VITS** | 음성 latent z에 대한 정규성 유지                         |
| **Glow-TTS** | flow 이전의 latent z 분포를 N(0,1)로 맞추기 위해 사용  |

---

## 🧠 VITS에서 KL Divergence란?

VITS는 인코더가 뽑은 **latent z (posterior)**와  
텍스트를 통해 예측한 **prior** 분포가 일치하도록 학습합니다.

```math
L_{KL} = D_{KL}(q(z|x) || p(z|y))
```

즉, "**텍스트로부터 예측한 분포와 실제 음성의 분포가 유사해야 한다**"는 것을 강제함.

---

## 📊 시각적 직관 (예시)

```
Posterior:    N(μ=0.5, σ=0.3)
Prior:        N(μ=0, σ=1.0)

→ 두 분포가 많이 다르면 KL ↑ → 패널티 ↑
→ 훈련 중 posterior가 prior에 가까워지도록 학습됨
```

---

## ✅ 핵심 요약

| 항목         | 설명                                         |
|--------------|----------------------------------------------|
| 정의         | 두 확률분포 간의 차이 측정                   |
| 사용 이유     | posterior와 prior의 차이를 줄이기 위해       |
| 성질         | 비대칭, 항상 0 이상, 같으면 0                |
| TTS 활용     | VITS, VAE, DiffTTS 등 latent alignment 학습    |

---

> 🎯 KL Divergence는 **확률적 인코딩**을 사용하는 모든 TTS 모델에서  
> 학습의 안정성과 표현력 향상에 **핵심적인 역할**을 합니다.

# 🧠 latent z (posterior): TTS에서의 의미와 작동

---

## 📌 정의

**latent z**는 모델이 입력(예: 음성, 텍스트)을 통해  
**추상적 특징 공간(latent space)**으로 압축한 **잠재 변수**입니다.

- **Posterior**: 실제 입력 데이터를 기반으로 추정된 z  
  `q(z|x)` ← 인코더(음성 → z)
- **Prior**: 텍스트로 예측한 z의 분포  
  `p(z|y)` ← 텍스트(음소 등) → flow or 변환

---

## 🎯 예시: 음성 인코딩 → latent z 추출

```python
# 음성 x를 posterior 인코더로 인코딩
x = waveform_tensor  # shape: [B, T]
posterior_encoder = VITSPosteriorEncoder()

# 출력은 정규분포 파라미터
mu, logvar = posterior_encoder(x)  # shape: [B, D]

# reparameterization trick (sampling z from N(mu, var))
std = torch.exp(0.5 * logvar)
eps = torch.randn_like(std)
z = mu + std * eps  # [B, D] ← sampled latent
```

---

## 🔁 왜 latent z가 필요한가?

| 이유               | 설명 |
|--------------------|------|
| 🎨 **다양성 보존**   | z를 샘플링하면 음성 표현의 다양성 반영 가능 |
| 🧭 **정보 압축**     | 고차원 waveform을 저차원 공간에 요약 |
| 🧬 **유사성 보존**   | 비슷한 z끼리는 비슷한 음성 의미를 가짐 |
| 🔄 **Flow/Decoder 입력** | z를 기반으로 오디오 생성 가능 |

---

## 🧠 posterior vs prior (VITS 기준)

| 항목         | 정의                                | 생성 방식     |
|--------------|-------------------------------------|----------------|
| **Posterior**| 실제 음성 x로부터 얻은 z (인코더)   | `q(z|x)`       |
| **Prior**    | 텍스트 y로부터 예측한 z 분포       | `p(z|y)`       |

→ 학습 시 두 분포가 **가깝도록 KL Divergence로 정규화**함

---

## 🎨 직관적 그림

```text
Text y ──→ Prior p(z|y) ──→ [샘플링된 z] ──→ Decoder → waveform
               ↑
            가까워지도록 KL Loss
               ↑
Waveform x ──→ Posterior q(z|x)
```

---

## 🔍 z가 변하면?

- 같은 텍스트라도 z를 다르게 샘플링하면 음성이 다르게 나옴
- 스타일, 감정, 억양 등에 영향을 줄 수 있음

```python
z1 = mu + std * torch.randn_like(std)
z2 = mu + std * torch.randn_like(std)

wave1 = decoder(z1)
wave2 = decoder(z2)
# → 동일 텍스트인데 발음 방식이 달라질 수 있음
```

---

## ✅ 정리

| 항목           | 설명                                      |
|----------------|-------------------------------------------|
| latent z       | 텍스트/음성 의미를 담는 잠재 변수         |
| posterior      | 실제 음성에서 추출한 z (`q(z|x)`)         |
| prior          | 텍스트에서 예측한 z 분포 (`p(z|y)`)       |
| 사용 목적      | 다양성, 표현력, 정보 압축, 자연스러운 생성 |
| 학습 방식      | KL divergence로 posterior와 prior 정렬     |

---

> 🎯 TTS에서의 latent z는  
> 단순한 임베딩이 아닌 **“음성의 확률적 본질”을 표현하는 핵심 변수**입니다.


---

# 📌 2. Glow-TTS (Flow 기반)

> Non-autoregressive + alignment-free + invertible model

### 🔁 구조

```text
Text (phoneme) → Prior flow → z
↓                      ↑
Mel decoder (inverse flow)
↓
Waveform (via vocoder)
```

### ✅ 특징

- `Flow` 모델을 통해 직접 mel-spectrogram을 생성
- alignment-free training (CTC나 attention 불필요)
- text → mel 변환이 병렬적, 추론 빠름

---

# 📌 3. Grad-TTS (Diffusion 기반)

> Reverse diffusion으로 mel-spectrogram을 점진적으로 정제

### ⏱️ 구조

```text
Text → duration predictor → alignment
↓
Gaussian noise z₀
↓ (Diffusion step by step)
Mel-spectrogram
↓
Vocoder → waveform
```

### ✅ 특징

- **Diffusion model**: `z₀`에서 노이즈 제거하며 mel 생성
- Prosody가 자연스럽고 품질 높음
- 느린 속도는 단점

---

# 📌 4. DiffTTS

> Grad-TTS의 개선형. 속도 + 품질 개선

- Text → duration predictor + noise schedule
- Diffusion step을 줄이고 속도 향상
- 멀티스피커, 멀티스타일에 확장 쉬움

---

# 🧪 공통 작동 흐름 예시 (VITS 계열 기준)

```python
text = "Hello world"
phoneme_seq = tokenizer(text)  # e.g. ['HH', 'EH', 'L', 'OW']
phoneme_tensor = to_tensor(phoneme_seq).unsqueeze(0).cuda()

# model: pretrained VITS or Grad-TTS
with torch.no_grad():
    waveform = model.infer(phoneme_tensor)  # [B, T]

# Save
sf.write("tts.wav", waveform.cpu().numpy(), samplerate=22050)
```

---

# 🧠 End-to-End 모델의 주요 장점

| 항목         | 설명                                           |
|--------------|------------------------------------------------|
| 음질         | GAN, Flow, Diffusion 도입으로 WaveNet 수준 가능 |
| 속도         | Glow-TTS, DiffTTS는 병렬 가능 → 빠름           |
| 통합 구조     | 한 모델로 학습 및 추론 → 관리 용이              |
| alignment    | Attention, CTC 없이 학습 가능                   |

---

# 🔚 정리 요약

| 모델     | 특징 요약                                 |
|----------|--------------------------------------------|
| **VITS** | VAE + Flow + HiFi-GAN 통합 / 고품질 / 무정렬 |
| **Glow-TTS** | Flow + 병렬 + alignment-free              |
| **Grad-TTS** | diffusion 기반 / 자연스러운 운율 / 느림     |
| **DiffTTS** | 개선된 diffusion / 속도 향상 / 고품질       |

---

> 📌 End-to-End 모델은 TTS의 최종 진화 단계로,  
> 품질·속도·운율·스타일 모든 면에서 향상된 현대식 TTS의 핵심입니다.
