## 🎯 Stage 1: Text → Acoustic Feature

---

## 📌 1. Acoustic Feature란?

**Acoustic Feature (음향 특성)** 는  
텍스트/음소를 실제 음성으로 변환하기 위해 필요한 **중간 표현 (intermediate representation)**입니다.

- 텍스트 → (음소) → **Acoustic Feature** → (vocoder) → Waveform
- **Waveform 이전 단계의 음성 정보**를 담고 있음
- 대부분 **Mel-spectrogram** 형태로 사용됨

---

## 🔉 자주 쓰이는 Acoustic Features

| Feature 종류         | 설명                                              | 차원 예시 |
|----------------------|---------------------------------------------------|-----------|
| **Mel-Spectrogram**   | 인간 청각 기반 필터 뱅크 적용한 주파수-시간 표현 | (80 x T)  |
| Linear Spectrogram   | STFT 결과 그대로 사용 (고해상도)                  | (1025 x T) |
| Pitch (F0)           | 기본 주파수 (소리의 높낮이)                        | (1 x T)   |
| Energy               | 소리의 세기 (진폭 에너지)                         | (1 x T)   |
| Duration             | 각 음소의 지속 시간                                | (음소 수,) |

> 대부분의 TTS는 Mel-spectrogram을 기본 acoustic feature로 사용합니다.

---

## 📦 2. 예시: 텍스트 → Mel-spectrogram

### 예시 입력

```text
텍스트: "사랑해"
음소 시퀀스: [s, a, r, a, ŋ, h, ɛ]
```

### 예측 결과 (Mel-spectrogram)

```python
# PyTorch Tensor 형식 (예시)
mel_pred.shape → torch.Size([80, 120])  # 80 Mel bins × 120 frame steps

# 일부 값 예시
mel_pred[:, :3] =
tensor([[ -4.2, -3.9, -3.5],
        [ -2.1, -1.7, -1.3],
        ...
        [ -6.7, -6.8, -6.5]])  # 총 80개 row
```

- `80`: mel frequency bin 수
- `120`: 시간 축 프레임 수 (예: 12kHz sampling 기준 약 2초)

### 시각화 (matplotlib 사용)

```python
import matplotlib.pyplot as plt
import librosa.display

plt.figure(figsize=(10, 4))
librosa.display.specshow(mel_pred.numpy(), sr=22050, hop_length=256, x_axis='time', y_axis='mel')
plt.colorbar(format="%+2.0f dB")
plt.title("Predicted Mel-Spectrogram")
plt.tight_layout()
plt.show()
```

---

## 📚 모델별 처리 방식

| 모델            | 구조                           | 특징                                           |
|-----------------|--------------------------------|------------------------------------------------|
| **Tacotron 1/2** | seq2seq + attention            | autoregressive, attention으로 시간 정렬 학습     |
| **FastSpeech 1/2** | duration + Transformer encoder | parallel, 빠르고 안정적인 mel 예측             |

---

## 🔁 전체 처리 흐름

```text
텍스트 → 음소 시퀀스 → 임베딩 벡터 시퀀스
   ↓
(Encoder → Attention → Decoder)
   ↓
Mel-spectrogram (80 x T)
   ↓
(Vocoder)
   ↓
Waveform (.wav)
```

---

## 🧠 요약

- **Acoustic Feature**는 텍스트와 오디오 사이를 연결하는 핵심 중간 표현입니다.
- Mel-spectrogram은 **사람이 인식하는 주파수 정보**를 효과적으로 표현하며,  
  대부분의 TTS 모델의 **예측 목표(output target)** 입니다.
- 좋은 acoustic feature 예측은 **자연스럽고 명료한 음성을 위한 기반**입니다.
