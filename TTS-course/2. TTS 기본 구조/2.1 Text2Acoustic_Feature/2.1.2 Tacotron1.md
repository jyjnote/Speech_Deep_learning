# 🎙️ Tacotron 1: End-to-End Speech Synthesis (2017)

---

## 📌 개요

**Tacotron 1**은 2017년 Google에서 발표한 **End-to-End TTS 모델**입니다.  
텍스트 입력을 **Mel-spectrogram + Linear spectrogram**으로 예측하고,  
**Griffin-Lim 알고리즘**을 이용해 waveform을 복원합니다.

> 🔑 핵심:  
> "문장 하나 → 음성 하나"를 별도의 복잡한 파이프라인 없이  
> 딥러닝 하나로 해결한 첫 성공 사례

---

## 🧬 전체 구조

```text
텍스트 → 임베딩 → Encoder (CBHG) → Attention → Decoder (LSTM)
    → Mel-Spectrogram → Postnet → Linear Spectrogram → Griffin-Lim → Waveform
```

---

## 📦 주요 구성 요소

### 🔤 1. Character Embedding
- 글자(character)를 정수로 매핑 후 임베딩
- 임베딩 차원: 보통 256차원

---

### 📚 2. Encoder: **CBHG 모듈**
> Conv Bank + Highway + BiGRU

| 구성               | 설명                                                  |
|--------------------|-------------------------------------------------------|
| Convolution Bank   | 여러 커널 크기의 1D CNN으로 로컬 패턴 추출             |
| Highway Network    | 복잡한 특성 통과 학습 가능                             |
| Bidirectional GRU  | 양방향으로 문맥 정보 처리                              |

- 입력: 임베딩된 character 시퀀스 `(T, D)`
- 출력: contextualized hidden states `(T, H)`

---

### 🎯 3. Attention (Bahdanau)
- Encoder → Decoder 간 soft alignment 학습
- Decoder의 각 시점마다 어디를 집중할지 결정

---

### 🔁 4. Decoder (Autoregressive)
- LSTM 기반 autoregressive decoder
- 이전 프레임의 mel-spectrogram을 받아 다음 프레임 예측
- teacher forcing 적용

---

### 🎨 5. Postnet + Linear Spectrogram
- CNN 기반 Postnet으로 mel 예측 보정
- Linear spectrogram 생성 (STFT 결과 복원 목적)

---

### 🔊 6. Griffin-Lim Vocoder
- Phase가 없는 linear spectrogram으로부터
- 반복적인 ISTFT → STFT → 보정 방식으로 waveform 복원

---

## 📈 작동 예시: "사랑해" 문장 예측

### 입력

```text
텍스트: "사랑해"
→ character 시퀀스: ['ㅅ', 'ㅏ', 'ㄹ', 'ㅏ', 'ㅇ', 'ㅎ', 'ㅐ']
→ 임베딩 → CBHG → context vectors
```

### 디코딩

- Attention이 'ㅅ'에 집중하며 mel 생성 시작
- LSTM이 하나씩 autoregressive하게 mel frame 생성
- 80차원 mel-spectrogram 생성: `(80, T)`

### 시각화

```python
import matplotlib.pyplot as plt
import librosa.display

plt.figure(figsize=(10, 4))
librosa.display.specshow(mel_pred, sr=22050, hop_length=256, x_axis='time', y_axis='mel')
plt.colorbar()
plt.title("Tacotron 1: Mel-Spectrogram for '사랑해'")
plt.tight_layout()
plt.show()
```

---

## 🛠️ Loss Function

```text
Loss = L1Loss(mel_pred, mel_target)
     + L1Loss(linear_pred, linear_target)
     + Guided Attention Loss (정렬 유도)
```
# 🧪 Tacotron 1 작동 예시: "사랑해" 문장을 음성으로

---

## 📌 입력: "사랑해"

- 글자 단위 character 시퀀스  
  `"사", "랑", "해"` → 자모 분리 시 `"ㅅ", "ㅏ", "ㄹ", "ㅏ", "ㅇ", "ㅎ", "ㅐ"`

- 각 음소는 임베딩되어 고정된 크기의 벡터로 변환됨  
  예: `"ㅅ"` → `[0.15, -0.3, ..., 0.04]` (256차원)

---

## ⚙️ Step-by-Step 작동 과정

### 1️⃣ **Character Embedding**
```text
입력: ["ㅅ", "ㅏ", "ㄹ", "ㅏ", "ㅇ", "ㅎ", "ㅐ"]
→ 7개 음소, 임베딩 차원 256
→ 텐서 shape: (T=7, D=256)
```

---

### 2️⃣ **Encoder: CBHG 모듈**

CBHG는 `Conv Bank + Highway + BiGRU` 로 구성

- CNN은 다양한 필터 크기로 지역 정보를 추출 (예: 자음/모음 조합)
- Highway는 정보 흐름을 학습
- BiGRU는 시간 방향으로 양방향 문맥 학습

```text
입력: 임베딩된 시퀀스 (7, 256)
출력: context vector 시퀀스 (7, 256)
```

---

### 3️⃣ **Attention Mechanism**

- Decoder가 어느 입력 음소를 참조할지 soft하게 결정  
- 예를 들어, `"사"`를 발음할 때 `"ㅅ"` + `"ㅏ"` 음소에 높은 attention

```text
t=1: Attention on "ㅅ"
t=2: Attention on "ㅏ"
t=3: Attention on "ㄹ"
...
→ 정렬이 soft하게 학습됨 (heatmap 가능)
```

---

### 4️⃣ **Decoder (Autoregressive)**

- LSTM 구조로 프레임 단위 mel-spectrogram을 하나씩 예측  
- 이전 출력(mel) + attention context를 받아 다음 프레임 생성

```text
예측 출력: mel-spectrogram shape = (80, T)
T = 예측된 프레임 수 (예: 100 프레임 = 약 2초)
```

---

### 5️⃣ **Postnet + Linear Spectrogram**

- Postnet: 5-layer CNN으로 mel 보정 (잔향/부드러움 개선)
- Linear Spectrogram: STFT 수준의 고해상도 예측 (1025 × T)

---

### 6️⃣ **Griffin-Lim Vocoder**

- linear spectrogram으로부터 **waveform 복원**
- 위상 정보를 iterative 방식으로 추정
- 최종 `.wav` 생성

---

## 📊 예시 결과 시각화

```python
# mel_output.shape = [80, 100]  (예: 100 프레임, 80 mel bins)

import matplotlib.pyplot as plt
import librosa.display

plt.figure(figsize=(10, 4))
librosa.display.specshow(mel_output, sr=22050, hop_length=256, x_axis='time', y_axis='mel')
plt.title("Tacotron 1: Mel-Spectrogram of '사랑해'")
plt.colorbar()
plt.tight_layout()
plt.show()
```

---

## ✅ 최종 출력 요약

| 단계        | 결과                                      |
|-------------|-------------------------------------------|
| 입력        | "사랑해" (character sequence)             |
| 중간 출력   | Mel-spectrogram (80 × T)                  |
| 후처리      | Linear spectrogram (1025 × T)             |
| 음성 복원   | Waveform (`.wav`) via Griffin-Lim         |

---
# 🧠 Tacotron 1의 핵심: Highway + BiGRU 완전 이해

CBHG 모듈은 다음 세 가지로 구성됩니다:

```
Conv Bank → Highway Network → BiGRU
```

이 중에서 **Highway**와 **BiGRU**는 "의미 추출 + 시퀀스 문맥 이해"에 핵심적 역할을 합니다.

---

## 1️⃣ Highway Network

### 📌 정의
> **입력 벡터 x**가 정보의 일부는 그대로 통과하고, 일부는 변형되도록  
> **Gate 구조**를 가진 비선형 레이어입니다. (Residual + Gate 개념)

### 💡 수식

```text
H(x) = ReLU(W_H x + b_H)         → 변환 정보
T(x) = sigmoid(W_T x + b_T)      → 얼마나 통과시킬지 (Gate)
Output = H(x) * T(x) + x * (1 - T(x))
```

### 🎯 왜 사용하나?
- 정보 손실 없이 깊은 네트워크 학습 가능
- 일부 정보는 그대로 "고속도로(highway)"로 통과
- CNN으로 추출된 국소 정보를 고차원적으로 통합

---

### 🧪 예시

```text
입력 벡터 x = [0.2, -0.5, 0.1, 0.8]

H(x) = [0.3, 0.1, 0.0, 0.9]   (ReLU)
T(x) = [0.9, 0.1, 0.5, 0.2]   (sigmoid)

Output = H * T + x * (1 - T)
       ≈ [0.29, -0.44, 0.05, 0.74]  ← 고속도로 통과 + 일부 처리
```

---

## 2️⃣ BiGRU (Bidirectional GRU)

### 📌 정의
> GRU (Gated Recurrent Unit)를 양방향으로 처리하여  
> **이전/이후 문맥 정보를 모두 고려**하는 순환 신경망

### 💡 구조

```
→ GRU: 현재 입력 + 과거 기억
← GRU: 현재 입력 + 미래 정보
↓
[양방향 GRU 출력 = Forward + Backward concat]
```

---

### 🎯 왜 사용하나?

- 단방향 RNN은 **이전 정보만** 반영 → 한계 있음  
- BiGRU는 **과거와 미래 정보를 동시에** 고려 가능  
- 텍스트의 문맥적 의미를 더 풍부하게 추출

---

### 🧪 예시

```text
입력 시퀀스 (음소 임베딩): ["ㅅ", "ㅏ", "ㄹ", "ㅏ", "ㅇ"]

Forward GRU:
  t=1 → [0.1, 0.2]
  t=2 → [0.3, 0.1]
  ...

Backward GRU:
  t=5 → [0.0, 0.5]
  t=4 → [0.2, 0.4]
  ...

최종 출력 (BiGRU):
  t=1 → [F=0.1,0.2] + [B=0.2,0.4] = [0.1,0.2,0.2,0.4]
  ...
→ shape: [T=5, D=hidden_dim * 2]
```

---

## ✅ CBHG 전체 흐름 예시 (입력: "사랑해")

```text
1. 텍스트: "사랑해"
2. 임베딩: (T=7, D=256)

3. Conv Bank → CNN으로 각 음소 지역 패턴 추출
4. Highway → 일부 정보는 그대로 통과, 일부는 변형
5. BiGRU → 이전 + 이후 문맥 고려해 sequence feature 완성

출력: (T=7, D=256) → attention으로 전달
```

---

## 🧠 요약

| 모듈         | 역할                                        |
|--------------|---------------------------------------------|
| Highway      | 정보 손실 없이 딥 네트워크 학습 (gate 통제)  |
| BiGRU        | 시간 방향 문맥 이해 (양방향 RNN)              |
| 전체 효과     | 로컬 + 문맥 정보 모두 담긴 context vector 생성 |

> ✅ CBHG는 Tacotron 1의 Encoder에서  
> **로컬-글로벌 정보**를 모두 담은 강력한 특성 인코더입니다.

## 🧠 정리

> Tacotron 1은 텍스트를 음성으로 변환하는 **완전한 End-to-End 시스템**으로,  
> attention 기반 decoder가 frame-by-frame으로 음성을 생성하며,  
> 최종 음성은 Griffin-Lim vocoder로 복원됩니다.


---

## ⚠️ Tacotron 1의 한계점

| 한계                     | 설명                                            |
|--------------------------|-------------------------------------------------|
| Griffin-Lim의 저품질     | 위상 정보 없음 → 음질 저하                     |
| Autoregressive 느림       | 프레임 단위 예측 → 실시간 불가                  |
| Alignment collapse 위험  | Attention이 한 글자에 집착하거나 무너짐         |

---

## 🧠 요약

| 항목             | 내용                                                      |
|------------------|-----------------------------------------------------------|
| 입력             | 텍스트 (character)                                        |
| 구조             | Embedding → CBHG → Attention → Decoder                    |
| 출력             | Mel + Linear Spectrogram                                  |
| Vocoder          | Griffin-Lim                                               |
| 장점             | End-to-End 구조, 직관적 설계                             |
| 단점             | 음질 낮음, 느림, 불안정                                   |

---

## 📘 논문 정보

- **Title:** *Tacotron: Towards End-to-End Speech Synthesis*
- **Link:** https://arxiv.org/abs/1703.10135
- **Code (Unofficial):** https://github.com/keithito/tacotron

---

> ✅ Tacotron 1은 TTS를 딥러닝 하나로 통합한 역사적인 모델이며,  
> 이후 Tacotron 2 → FastSpeech → VITS 등으로 진화하는 핵심 기반이 되었습니다.

